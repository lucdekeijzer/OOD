{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_ood"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cus1953kCP1c",
        "outputId": "f2800ec5-87a3-47f6-bbae-10a814f0e439"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_ood\n",
            "  Downloading pytorch_ood-0.2.1-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_ood) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_ood) (0.20.1+cu121)\n",
            "Collecting torchmetrics>=1.0.0 (from pytorch_ood)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_ood) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_ood) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->pytorch_ood) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->pytorch_ood) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0->pytorch_ood) (1.3.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=1.0.0->pytorch_ood) (24.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics>=1.0.0->pytorch_ood)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.12.0->pytorch_ood) (11.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics>=1.0.0->pytorch_ood) (75.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.0->pytorch_ood) (3.0.2)\n",
            "Downloading pytorch_ood-0.2.1-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_ood\n",
            "Successfully installed lightning-utilities-0.11.9 pytorch_ood-0.2.1 torchmetrics-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset, DataLoader, random_split\n",
        "import numpy as np\n",
        "from pytorch_ood.model import WideResNet\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from collections import Counter\n",
        "from pytorch_ood.detector import MultiMahalanobis, MCD\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Configuration and hyperparameters\n",
        "RANDOM_SEED = 42\n",
        "DEPTH = 28\n",
        "WIDEN_FACTOR = 10\n",
        "DROPOUT_RATE = 0.3\n",
        "EXCLUDE_CLASS = 'airplane'\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.1\n",
        "BATCH_SIZE = 128\n",
        "TEST_BATCH_SIZE = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "FILE_PATH = ' '\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# CIFAR10 class names\n",
        "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "4nHiszs_CRR-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions\n",
        "\n",
        "Run these functions"
      ],
      "metadata": {
        "id": "aXpYVoXOBdK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_wHMnjbhBTNN"
      },
      "outputs": [],
      "source": [
        "def count_samples_per_class(dataset, classes, exclude_class):\n",
        "  \"\"\"\n",
        "  Count the number of samples for each class in the dataset, excluding a specified class.\n",
        "\n",
        "  Args:\n",
        "      dataset: The input dataset to analyze\n",
        "      classes: List of all possible classes\n",
        "      exclude_class: The class to exclude from counting\n",
        "\n",
        "  Returns:\n",
        "      dict: Dictionary with class names as keys and sample counts as values\n",
        "  \"\"\"\n",
        "  filtered_classes = [c for c in classes if c != exclude_class]\n",
        "  class_counts = {cls: 0 for cls in filtered_classes}\n",
        "\n",
        "  for _, label in dataset:\n",
        "      # Adjust label index to account for excluded class\n",
        "      adjusted_label = label if label < classes.index(exclude_class) else label - 1\n",
        "      class_counts[filtered_classes[adjusted_label]] += 1\n",
        "\n",
        "  return class_counts\n",
        "\n",
        "def setup_data_loaders_cifar_10(exclude_class):\n",
        "  \"\"\"\n",
        "  Set up PyTorch data loaders for CIFAR-10 dataset with one class excluded.\n",
        "  Handles data transformation, class exclusion, and label remapping.\n",
        "\n",
        "  Args:\n",
        "      exclude_class: The class to exclude from the dataset\n",
        "\n",
        "  Returns:\n",
        "      tuple: (train_loader, val_loader, test_loader, exclude_index)\n",
        "          - train_loader: DataLoader for training data\n",
        "          - val_loader: DataLoader for validation data\n",
        "          - test_loader: DataLoader for test data\n",
        "          - exclude_index: Index of the excluded class\n",
        "  \"\"\"\n",
        "  # Define standard CIFAR-10 transformations\n",
        "  transform = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "  ])\n",
        "\n",
        "  # Load full datasets\n",
        "  full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "  full_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "  # Get index of class to exclude\n",
        "  exclude_index = CLASSES.index(exclude_class)\n",
        "\n",
        "  # Create label mapping for remaining classes\n",
        "  label_mapping = {}\n",
        "  new_label = 0\n",
        "  for old_label in range(len(CLASSES)):\n",
        "      if old_label != exclude_index:\n",
        "          label_mapping[old_label] = new_label\n",
        "          new_label += 1\n",
        "\n",
        "  # Custom dataset class for handling label remapping\n",
        "  class RemappedSubset(torch.utils.data.Dataset):\n",
        "      \"\"\"Custom dataset class that remaps class labels after excluding a class\"\"\"\n",
        "      def __init__(self, dataset, indices, mapping):\n",
        "          self.dataset = dataset\n",
        "          self.indices = indices\n",
        "          self.mapping = mapping\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          image, label = self.dataset[self.indices[idx]]\n",
        "          return image, self.mapping[label]\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.indices)\n",
        "\n",
        "  # Filter out excluded class and create new datasets\n",
        "  train_indices = [i for i, (_, label) in enumerate(full_trainset) if label != exclude_index]\n",
        "  test_indices = [i for i, (_, label) in enumerate(full_testset) if label != exclude_index]\n",
        "\n",
        "  train_subset = RemappedSubset(full_trainset, train_indices, label_mapping)\n",
        "  test_subset = RemappedSubset(full_testset, test_indices, label_mapping)\n",
        "\n",
        "  # Split training data into training and validation sets\n",
        "  train_size = int(len(train_subset) * (1 - VALIDATION_SPLIT))\n",
        "  val_size = len(train_subset) - train_size\n",
        "  train_subset, val_subset = random_split(train_subset, [train_size, val_size])\n",
        "\n",
        "  # Create and return data loaders\n",
        "  train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "  val_loader = DataLoader(val_subset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "  test_loader = DataLoader(test_subset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "  return train_loader, val_loader, test_loader, exclude_index\n",
        "\n",
        "def create_model():\n",
        "  \"\"\"\n",
        "  Create and initialize a WideResNet model for classification.\n",
        "\n",
        "  Returns:\n",
        "      tuple: (model, device)\n",
        "          - model: Initialized WideResNet model\n",
        "          - device: torch device (cuda if available, else cpu)\n",
        "  \"\"\"\n",
        "  model = WideResNet(\n",
        "      depth=DEPTH,\n",
        "      num_classes=9,  # One less than CIFAR-10 due to excluded class\n",
        "      widen_factor=WIDEN_FACTOR,\n",
        "      drop_rate=DROPOUT_RATE\n",
        "  )\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = model.to(device)\n",
        "  return model, device\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, device, exclude_index):\n",
        "  \"\"\"\n",
        "  Train the model and perform validation.\n",
        "\n",
        "  Args:\n",
        "      model: The model to train\n",
        "      train_loader: DataLoader for training data\n",
        "      val_loader: DataLoader for validation data\n",
        "      criterion: Loss function\n",
        "      optimizer: Optimization algorithm\n",
        "      device: torch device\n",
        "      exclude_index: Index of excluded class\n",
        "\n",
        "  Returns:\n",
        "      float: Validation loss\n",
        "  \"\"\"\n",
        "  # Training phase\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      train_total += targets.size(0)\n",
        "      train_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  # Validation phase\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs, targets in val_loader:\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          val_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          val_total += targets.size(0)\n",
        "          val_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  # Print training and validation statistics\n",
        "  print(f'Train Loss: {train_loss/len(train_loader):.3f} | '\n",
        "        f'Train Acc: {100.*train_correct/train_total:.3f}%')\n",
        "  print(f'Val Loss: {val_loss/len(val_loader):.3f} | '\n",
        "        f'Val Acc: {100.*val_correct/val_total:.3f}%')\n",
        "\n",
        "  return val_loss / len(val_loader)\n",
        "\n",
        "def create_ood_loader(exclude_class, batch_size=128):\n",
        "  \"\"\"\n",
        "  Create a DataLoader for out-of-distribution (OOD) data using the excluded class.\n",
        "\n",
        "  Args:\n",
        "      exclude_class: The class to use as OOD data\n",
        "      batch_size: Batch size for the DataLoader\n",
        "\n",
        "  Returns:\n",
        "      DataLoader: DataLoader containing only samples from the excluded class\n",
        "  \"\"\"\n",
        "  # Define transformations matching the main dataset\n",
        "  transform = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "  ])\n",
        "\n",
        "  full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "  exclude_index = CLASSES.index(exclude_class)\n",
        "\n",
        "  # Custom dataset for OOD samples\n",
        "  class OODDataset(torch.utils.data.Dataset):\n",
        "      \"\"\"Dataset containing only samples from the excluded class\"\"\"\n",
        "      def __init__(self, dataset, exclude_index):\n",
        "          self.dataset = dataset\n",
        "          self.indices = [i for i, (_, label) in enumerate(dataset) if label == exclude_index]\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          return self.dataset[self.indices[idx]]\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.indices)\n",
        "\n",
        "  ood_dataset = OODDataset(full_trainset, exclude_index)\n",
        "  return DataLoader(ood_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "def get_detector(detector_name, model, device, train_loader, val_loader, test_loader):\n",
        "  \"\"\"\n",
        "  Create and initialize an OOD detector based on the specified method.\n",
        "\n",
        "  Args:\n",
        "      detector_name: Name of the detector ('Mahalanobis' or 'Monte Carlo')\n",
        "      model: Trained model\n",
        "      device: torch device\n",
        "      train_loader: DataLoader for training data\n",
        "      val_loader: DataLoader for validation data\n",
        "      test_loader: DataLoader for test data\n",
        "\n",
        "  Returns:\n",
        "      detector: Initialized OOD detector\n",
        "  \"\"\"\n",
        "  if detector_name == 'Mahalanobis':\n",
        "      # Extract layers for Mahalanobis detector\n",
        "      layer1 = model.conv1\n",
        "      layer2 = model.block1\n",
        "      layer3 = model.block2\n",
        "      layer4 = model.block3\n",
        "\n",
        "      class MyLayer(nn.Module):\n",
        "          def __init__(self, bn1, relu):\n",
        "              super(MyLayer, self).__init__()\n",
        "              self.bn1 = bn1\n",
        "              self.relu = relu\n",
        "\n",
        "          def forward(self, x):\n",
        "              x = self.bn1(x)\n",
        "              x = self.relu(x)\n",
        "              return x\n",
        "\n",
        "      layer5 = MyLayer(model.bn1, model.relu)\n",
        "      detector = MultiMahalanobis([layer1, layer2, layer3, layer4, layer5])\n",
        "      detector.fit(train_loader, device=device)\n",
        "      return detector\n",
        "\n",
        "  elif detector_name == \"Monte Carlo\":\n",
        "    #Amount of samples can be changed for more robust performance. The more samples, the better the detector\n",
        "      return MCD(model, samples=30)\n",
        "  else:\n",
        "      raise ValueError(f\"Unknown detector name: {detector_name}\")\n",
        "\n",
        "def run_and_visualize(detector, test_loader, ood_loader, device, visualization=False, plot_scores=False):\n",
        "  \"\"\"\n",
        "  Run OOD detection on test and OOD data and visualize results.\n",
        "\n",
        "  Args:\n",
        "      detector: OOD detector\n",
        "      test_loader: DataLoader for in-distribution test data\n",
        "      ood_loader: DataLoader for out-of-distribution data\n",
        "      device: torch device\n",
        "      visualization: If True, print example predictions\n",
        "      plot_scores: If True, plot score distributions\n",
        "\n",
        "  Returns:\n",
        "      tuple: (id_scores, id_labels, ood_scores, ood_labels)\n",
        "  \"\"\"\n",
        "  id_scores = []\n",
        "  id_labels = []\n",
        "  ood_scores = []\n",
        "  ood_labels = []\n",
        "\n",
        "  # Process in-distribution data\n",
        "  print(\"Processing In-Distribution Data:\")\n",
        "  for i, (x, y) in enumerate(test_loader):\n",
        "      result = detector.predict(x.to(device))\n",
        "      id_scores.extend(result.cpu().numpy())\n",
        "      id_labels.extend(y.cpu().numpy())\n",
        "\n",
        "      if visualization and i < 10:\n",
        "          for j in range(min(len(y), 10)):\n",
        "              print(f\"Target Label: {y[j]}, OOD Score: {result[j]:.4f}\")\n",
        "\n",
        "  # Process out-of-distribution data\n",
        "  print(\"Processing Out-of-Distribution Data:\")\n",
        "  for i, (x, y) in enumerate(ood_loader):\n",
        "      result = detector.predict(x.to(device))\n",
        "      ood_scores.extend(result.cpu().numpy())\n",
        "      ood_labels.extend(y.cpu().numpy())\n",
        "\n",
        "      if visualization and i < 10:\n",
        "          for j in range(min(len(y), 10)):\n",
        "              print(f\"Target Label: {y[j]} (OOD), OOD Score: {result[j]:.4f}\")\n",
        "\n",
        "  if plot_scores:\n",
        "      plot_score_distributions(id_scores, ood_scores)\n",
        "\n",
        "  return id_scores, id_labels, ood_scores, ood_labels\n",
        "\n",
        "def plot_score_distributions(id_scores, ood_scores):\n",
        "  \"\"\"\n",
        "  Plot histograms comparing the distribution of OOD scores for in-distribution\n",
        "  and out-of-distribution data.\n",
        "\n",
        "  Args:\n",
        "      id_scores: List of scores for in-distribution samples\n",
        "      ood_scores: List of scores for out-of-distribution samples\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.hist(id_scores, bins=50, density=True, alpha=0.6, color='blue',\n",
        "            label='In-Distribution')\n",
        "  plt.hist(ood_scores, bins=50, density=True, alpha=0.6, color='red',\n",
        "            label='Out-of-Distribution')\n",
        "\n",
        "  plt.xlabel('OOD Score')\n",
        "  plt.ylabel('Density')\n",
        "  plt.title('Distribution of OOD Scores')\n",
        "  plt.legend()\n",
        "\n",
        "  # Add mean lines\n",
        "  plt.axvline(np.mean(id_scores), color='blue', linestyle='dashed', alpha=0.5)\n",
        "  plt.axvline(np.mean(ood_scores), color='red', linestyle='dashed', alpha=0.5)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def single_image_inference(img_path, detector):\n",
        "  \"\"\"\n",
        "  Perform OOD detection on a single input image.\n",
        "\n",
        "  Args:\n",
        "      img_path: Path to the input image file\n",
        "      detector: OOD detector model\n",
        "  \"\"\"\n",
        "  # Load the image\n",
        "  file_path = str(img_path)\n",
        "  custom_image = Image.open(file_path).convert('RGB')\n",
        "\n",
        "  # Create the transformation pipeline\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize((32, 32)),  # First resize to CIFAR-10 dimensions\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "  ])\n",
        "\n",
        "  # Apply transformation\n",
        "  custom_image_tensor = transform(custom_image)\n",
        "\n",
        "  # Add a batch dimension (CIFAR-10 batches are [N, 3, 32, 32])\n",
        "  custom_image_tensor = custom_image_tensor.unsqueeze(0)  # Shape: [1, 3, 32, 32]\n",
        "\n",
        "  # Ensure the device matches the detector\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  custom_image_tensor = custom_image_tensor.to(device)\n",
        "\n",
        "  # Perform prediction\n",
        "  result = detector.predict(custom_image_tensor)\n",
        "  print(f\"Prediction result for the custom image: {result}\")\n",
        "\n",
        "\n",
        "  # Display the image\n",
        "  plt.imshow(custom_image)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n"
      ],
      "metadata": {
        "id": "IWySWZc-ClWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader, exclude_index = setup_data_loaders_cifar_10(EXCLUDE_CLASS)\n",
        "\n",
        "# Print sample counts for training set\n",
        "print(\"Sample Counts per Class in Training Set:\")\n",
        "full_trainset = train_loader.dataset.dataset\n",
        "class_counts = count_samples_per_class(full_trainset, CLASSES, EXCLUDE_CLASS)\n",
        "for cls, count in class_counts.items():\n",
        "  print(f\"{cls}: {count}\")\n",
        "\n",
        "# Create model\n",
        "model, device = create_model()\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "print(f\"Training WideResNet (depth={DEPTH}, widen_factor={WIDEN_FACTOR}) \"\n",
        "  f\"on CIFAR10 excluding {EXCLUDE_CLASS} class\")\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "  # Train and validate for one epoch\n",
        "  val_loss = train(model, train_loader, val_loader, criterion, optimizer, device, exclude_index)\n",
        "\n",
        "  # Step the learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "  # Save best model based on validation loss\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), FILE_PATH)\n",
        "\n",
        "# Load best model for final testing\n",
        "model.load_state_dict(torch.load(FILE_PATH))\n",
        "\n",
        "\n",
        "\n",
        "# print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "print(f\"Best model saved as {FILE_PATH}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bO-wMgV3CnIG",
        "outputId": "ed0388be-190a-4bbc-e670-0017c396dda7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 72.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "\n",
            "Label Remapping Verification:\n",
            "Original labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Excluded class index: 0\n",
            "Label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8}\n",
            "\n",
            "New label assignments:\n",
            "automobile: 0\n",
            "bird: 1\n",
            "cat: 2\n",
            "deer: 3\n",
            "dog: 4\n",
            "frog: 5\n",
            "horse: 6\n",
            "ship: 7\n",
            "truck: 8\n",
            "\n",
            "Dataset sizes:\n",
            "Full training set (before exclusion): 50000\n",
            "Training set (after exclusion): 36000\n",
            "Validation set: 9000\n",
            "Test set (after exclusion): 9000\n",
            "\n",
            "Verifying labels in training set:\n",
            "Unique labels in training set: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "Sample Counts per Class in Training Set:\n",
            "automobile: 5000\n",
            "bird: 5000\n",
            "cat: 5000\n",
            "deer: 5000\n",
            "dog: 5000\n",
            "frog: 5000\n",
            "horse: 5000\n",
            "ship: 5000\n",
            "truck: 5000\n",
            "Training WideResNet (depth=28, widen_factor=10) on CIFAR10 excluding airplane class\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-829d705cd795>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# Train and validate for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# Step the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c83ea30d8f3b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, exclude_index)\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "d1xoUVelC1oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  model, device = create_model()\n",
        "\n",
        "  model.load_state_dict(torch.load(FILE_PATH, weights_only=True))\n",
        "\n",
        "\n",
        "  # Setup data loaders\n",
        "  train_loader, val_loader, test_loader, exclude_index = setup_data_loaders_cifar_10(EXCLUDE_CLASS)\n",
        "  ood_loader = create_ood_loader(EXCLUDE_CLASS)\n",
        "\n",
        "  # Create detector\n",
        "  detector = get_detector('Monte Carlo', model, device, train_loader, val_loader, test_loader)\n",
        "\n",
        "  # Run detection and visualization\n",
        "  id_scores, id_labels, ood_scores, ood_labels = run_and_visualize(\n",
        "      detector=detector,\n",
        "      test_loader=test_loader,\n",
        "      ood_loader=ood_loader,\n",
        "      device=device,\n",
        "      visualization=True,  # Set to False to skip example printing\n",
        "      plot_scores=True    # Set to False to skip distribution plotting\n",
        "  )\n",
        "  # single_image_inference(img_path=\"/content/dogetokenbase_32.png\", detector=detector)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "id": "S5XkgAMeC3pe",
        "outputId": "72c085e8-f42f-4233-8397-9a510f8474e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Label Remapping Verification:\n",
            "Original labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Excluded class index: 0\n",
            "Label mapping: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8}\n",
            "New label assignments:\n",
            "automobile: 0\n",
            "bird: 1\n",
            "cat: 2\n",
            "deer: 3\n",
            "dog: 4\n",
            "frog: 5\n",
            "horse: 6\n",
            "ship: 7\n",
            "truck: 8\n",
            "Dataset sizes:\n",
            "Full training set (before exclusion): 50000\n",
            "Training set (after exclusion): 36000\n",
            "Validation set: 9000\n",
            "Test set (after exclusion): 9000\n",
            "Verifying labels in training set:\n",
            "Unique labels in training set: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "Files already downloaded and verified\n",
            "OOD Dataset Statistics:\n",
            "Number of OOD samples: 1000\n",
            "OOD class: airplane (index 0)\n",
            "calculating variance\n",
            "Prediction result for the custom image: tensor([4.6580e-06])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFsxJREFUeJzt3EusZfl1F+D/eZ/7rndVp7vdr7LblaQ7doIVRxaQhwgCKVKkSJGYIsEUJkhIzJkghQGKEAMGDJggovCcIAECFBELBePYiuO23a2Oq7u63rfu49zz3IeJtaZZC/VNO+L7xqtW7bv3Pud39mD/etvtdtsAoLXW/6wPAIAfH0IBgCAUAAhCAYAgFAAIQgGAIBQACEIBgDDMDv7Hf/EbpcVvfu6l9Gx/eru0e3eSf99uvV6WdlcslxeXtnu8mpXmZ4uT0vyNm/nr8+Txg9LuyWKQnl0MD0q7t7svp2evvvxTpd29wU5pvus26dl+P39OWmut1+Xv8W6bP47WWptM83/nalO7D9e9/LEMt7Vz0rquNL7d5n/zdoXZ1lrrFd757ba1467oCvdJa629du/X/tQZTwoABKEAQBAKAAShAEAQCgAEoQBAEAoABKEAQBAKAAShAEAQCgCEdPdR1fFJvjNl/ej7pd03btX6cirWxc6hy1JubOpPSuOPnj7Lry4eyuBwLz179mhe2r23Wxju1qXdrVjFU1HpSWqt1q2zM6gd+PLpJ+nZbmdc2j0a5Y9722r3LH82PCkAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCAEEoABCEAgAhXXPx9IcflRbPnh9XjyVtfjFNz+7uVHoRWuv18wUT40mtJWRnnD/urlhbMd69Wpofjnrp2Ts3bpR2743yf+e3//e/K+2+Pc0fy1s/ma/baK2147NFaX4wLPym6rrS7qNJ/vqcPH5R2v2vf+ffpGdfup2/lq219td/82+kZ4/PVqXd/NnwpABAEAoABKEAQBAKAAShAEAQCgAEoQBAEAoABKEAQBAKAAShAEBIl/d8+MHHpcU3b7+Unr1+a7+0u9JndO3W7dLu8W6+6+Xa4aC0u3Qc41qXUdXFPN/xdHb8pLR7fp7v7al6cf+76dnTB6+UdvfHta6kiitHtXv8k/fzXWNf/59fL+2erI/Ts49+uCntns/m6dlBr9YdttluS/P8v/GkAEAQCgAEoQBAEAoABKEAQBAKAAShAEAQCgAEoQBAEAoAhPR75p+7+5XS4tt3jtKzd27XKh1ef/2N9Ox69by0e2d/lJ69OFuVdj9+9jA9++zR+6XdL17k6wVaa+18NkvPjgqVGK219uqt6+nZe3drVRQXs3zVwTd/7z+Xdh/eyB931cnJeWn+++99kJ4dDq6Udv+vP3gvPfu1X/7Z0u6uK1RRXF4byqXrtt3l7a6cw0vgSQGAIBQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYCQ7j66+869yzuIcb5vqLXWHj3O98j01rVj+Q+/m+/LuTg/Ke3em6ZPd2uTTWn3zv5eaf72nbvp2Vfv3CrtvjnJX8/F6rS0+/7976Rnu3Wts2l+VrsPR6PC9Syq3Cvf+u4PLu047rz8Wml+UDgni2Wt/KjX+2w7gf5/4UkBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAI6XfSZw+f1Rbv5F9379ZXS7v/2+//l/Tsl7/85dLu3uQwPzyrvXY/PbiRnz2alHbvX7lSmn/zJ382Pfv0/sel3U8fvkjP7k9Xpd2n81l69vN33y3tHo5rtRUf3L+fnj3a3Sntvvml/LEvTv+otHv9er7Oo7+al3aP+/nqioteV9r946Tfu8Tf0/3P9rx4UgAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQACCky14uPqn138yfHadnT69dKe2+fZDvkRlPah1Cb779U+nZ4XBc2j3c3UvP7hSPe7K7X5q/+RP30rP/5Ld/p7T7+Q++lZ79e3//b5Z2X7nzhfTsfLpb2v2D9z4qzY/GhWs0uyjtvl64t/7Kr/1iafcf/v4307Pf/+M/KO1+9xf+Qnp2uHeztHu9Lo1fqm0v3/HU29Y60j5rnhQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYCQrrl4+aXa4tMr+dqFJ0+flHa/dvdL6dnJ0dXS7lcPr6Rnl71ajUJ/nD7drVvW3unfO7pSO5ZNvnbhV3/p3dLu/V/9ufTseFyr55jsH6Vn/+U//1el3f/9W49L829/4bX07N/5279R2t2Wz9Kjz558Ulp95/X8Z+LZ985Lu2cvXqRn94s1F1XDfr5eYrHaFHeP0rNdq+3+rHlSACAIBQCCUAAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIOTLeKqLB91lrS6ZDvP9J621Nj85Ts/O+vPS7p2dfFfSRbH76NqNWo/Mg4en6dlV76C0e7Kf/zuXg0Fpd683Ts/+xV/5Wmn353+uV5q/+/YX07PX79wp7V6eTdOz4/WstHu9zPcZvfb6srR7Nc/vHtVOd1uuascyGE/Ss5Uuo9Za27T891vxz/zMeVIAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQACAIBQCCUAAgpLuPPn5+Ulp8MM3333zpK18t7T45yff2zE6PS7uXvXznzKPHf1La/eGDZ+nZr3y1dk4O9vZL8//oH/7j9Oxv/bN/X9r9l776cnr2n/72b5V271/dpGffOqx1Nt07qPUTnZ3lO4fm61qX1XT3Wnp2u873QbXW2rZQebY3elrafXb8OD17c5O/lq21Nip0GbXW2rortA71aw1Fgy7/e7prtb/zs+ZJAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGAIBQACOn33d/44s+UFp+f5WsxTuar0u6dQh3B6XxR2v2d7347PfvFt98t7T68l68uGPW70u7z43yFRmutvVjlaxfuvFyrF3j19bfTsx89qNUoHE3zdQQ7453S7rY4K43vTPP1Eher2u+v2SJfobE4Py/t3l48Ss+ezy5Ku48Ks+fHT0q7dw6vl+ZXhSqKfi9f/dFaa5tW+Hz2ahUabVsb/7R5UgAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQACCkCz9eeecXaptXy/To2cm8tHrb5fuMjj96Xtr9ypv30rPdcFDaPbvI/537B7U+qB8n0918V9LiPN+R1Vpr73+c78vZG+f7g1pr7daNW6X5Wct3H12m/b1ab89Fpc5oWftsXvQ36dnV/LS0e//gSml+WKgcWna1z1uvX/vsX5b+ttbtltr5qW8E4M8toQBAEAoABKEAQBAKAAShAEAQCgAEoQBAEAoABKEAQEi/H79YbEuLx8P99Oy1m1dKuyvuvPpOaX69zVcjnJxU+gJqNovz0nyvXzuWX/+rfzk9+8n9D0q7v/dHf5ie/bezB6XdFX/tF98tzXeba7X/oPKRGNYqMfan+Y6Gfrcu7V6vztKz3bpWFbKd5//O+Xm+sqS11tqtm7X57Sg9OhjUrk9X+zos6ffyVSGlezD7/3/6KwH480ooABCEAgBBKAAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIAId19NDtflhbPWn5+NOhKu/u9QXp2MDgu7R6O99Kzk0n69JVNDm+U5p88fVya/+Jbt9Oz/+Dv/q3S7vnFcXp2dpbv4WmttYP9/LUfDmq/eU4vTorHcpg/lumktHs9z3dfXTy/X9q9ePE8v/ui1ql10fLzu6e18/3mtNZP1AoVQvNevmuqtdb6l1E69CODLn/fDrv85yHLkwIAQSgAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCAEEoABDSPQ3ref7V+NZa6+dXt/VgVNo9HOZfSe9WpdVttc6/G78t1iiMS6/S117pX8xntfnTF+nZo93aK/3TXn5+3NV2rzaL9OzJRa2a5eqVl0rzk938fTs7qX1+Wpe/nsNuXlr9Yp6vl1gva9fn6Hq+nqUrfjjn89PSfBsdpEcHXaETo7U2315ezUXb5O/brnjcGZ4UAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGAIBQACOmCom5d6ynpWn6+K/TZtNbaajNIz1Y6mFprrb/K9470erXOplWhs6lquyyew8VxfrZ4fTab/G+N1bbW3VK7V2rn++TF09L87kX+PqwaFXq15v1paXc32EnP3nz5emn30ZV899Hk6Fpp97ZNSvObVf7z2XXr0u7xpliqVrDt8rOr4j2e4UkBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGAkC4GWs2LXR+DfIFHr1frJxpWykF6xW6dLt+vMhzWzsl6ne9i6a3OS7t3d2u9MN0632lz9vyT0u6TZ8/Ss4tuW9o9X85L8xWT6dXifOGcF2uSeoXfa/1J7Ry+8la+n+hg/6i0u1e4novK57i1tsjXkv3oYPKfz3XxAhW+3lrX1Q686y6vUyvDkwIAQSgAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCAEEoABDS/RKbVe1V+lZ4hX3Qq73uvloXKjSG1dzLvxpffR2931ukZ7fbWm3F7sFhaX48ztd/DIY7pd3DYb7OY7GsVQBMx9P07OG1fJVHa63t7NfmZxdn6dnlpnaPj6b5c3792iul3RW1kpjW+uv8v1icfVTavZoXK06G+WNZbWrfE6N+pZqn9j2x6tal+U+bJwUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQACAIBQBCusBjtS72cRTG1/18J1BrrQ36hUaWfJVRa6214SjfU7K6xIqSbrtfmh+vav0q61X+nA96td6r8STf23Ne7D6aTPLnZTTM9yS11trZWe0+bMO99Oj+Xu1Y5ot8z89g2SvtPti5vN+Ci8V5enZ2clravZrX5tvOjfToelO7x9eb/BfLqKvdV4ti79mnzZMCAEEoABCEAgBBKAAQhAIAQSgAEIQCAEEoABCEAgBBKAAQ0jUX3Sr/+vpl2wxqr6RXzC669Ozu3qi0+9reQXr28Oat0u7zxaw0v1hepGc321qNQkWltqK11kbjfAXAfFE77uGwVhXSNpW6ldo9e/XwKD27Xde6XLpC7UJvlb9PWmvt/PhpevbFi8el3beXb5Tmu0J9TrXmomJRqMRorbVWvA0/bZ4UAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGAIBQACOnuo/WPUfdRK1SJDAbpP7G11tqN69fTs9NRraTk8Fq++2hnP9/x01q9+6gV+lg261r/TcVkWvs7R8NpYbrWZzMuXs+Kk+Vp7R8Uepuu7NeOe3F2lp6dnT4p7X786OP07GY5L+1ermrzrb9Mj666y/t9vF3n+9Raa603LHRqXQJPCgAEoQBAEAoABKEAQBAKAAShAEAQCgAEoQBAEAoABKEAQCjUXORfjb9sm83lZdlLd27lh4fj0u7p9DA9O58XaxGK1ptFenazrtULLNb5eoHJcLe0e7vN7+4Vr8+21eo8Vv1Renayt1M8lnwNyfOzQu9La213kD/ug4P8Pdtaa+cvnqVnT4o1F+tVrf5h2fLfWb2WPydlq1rdyuWVreR4UgAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQACCku48WF8VemO15erZb1zpqNoXOlMF4Wtr94Ycfpmfffefd0u7FWb6L5ePnD0u7++P0pWyttTYZ9NKzm2HtHLZFvp9ota312azO8/fhaNzVdpfrb/LHsih26wyHB+nZyaTWlrPq8r8FD/evlXYfHeXv8WdPav1es+J30GaZPy+rbe1euUyjLt9Ldhk8KQAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIAQSgAEIQCACHdjTCfz0qLu03+tfH+5nlp93SYr3T4iau11/RX/W16ttvk6xxaa+20UNGwOM/XBbTW2vxJvlaktdaWF/lX6XujagVAvhaj29Re6R+NJunZ1bK2e7vJV3+01tqw0M4yaLV7ZbvMX8/e8EZpd7+f/zu3q/z5bq21zUW+tuT0rFZbcfi8do93o/yxdMUql/HgEn9PF2pILoMnBQCCUAAgCAUAglAAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAEK6ROijBw9Li7er8rGk3TwYpWe/8eT90u6X7txMz77xhXul3RWvvfRqaf74xdPS/OPnx+nZrru8i3k2L/6DQt9Uf1jr7RkXeq9aa23Y1fqMKtZdvlhpdX5S2r0/3c0fx8lxaffkNH9O7l77XGn34WCvNH9W+c27zvckVeefL2ufn3WhU2s4yV/LLE8KAAShAEAQCgAEoQBAEAoABKEAQBAKAAShAEAQCgAEoQBAEAoAhHT30ds//fOlxfNVvhtkNT8r7V6dPk/P1tpSWvuZr/1KevajJ89Ku4+u3ihM1/pSrhxdL81XvJjNSvObTf63xrRblHbfunErPTua1q7+/PxJaX7xIn/9Z+fHpd27k8P07GbVlXafFHqvdta90u6Kw25Qmj/oat1Uh4WOpzbN96lVjRe1bqrnZ/n5blUtD/vTeVIAIAgFAIJQACAIBQCCUAAgCAUAglAAIAgFAIJQACAIBQBCuubi3Z//pdLiwTZfc1G2ubi01eeLfKXDZP9qafd4Zyc9u1rWqguq5r3L+z1w8+ad9OytWzdLu7t+/hwuF7UKgAfFmovRtHAOz0ur23q9TM8e7ufPSWutvbjIV4v88XvvlXZPau0sJdPdo9L8S3du53ffzs+21trOUb6G5Pa4VkEznkzSs9v5p/8960kBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGAkO4+ulhcXqlJ+iB+pNfPd4NUDXfG6dnpMN9P01prp/NCr9K2ltfrZe1YBm03PXt09Upp9+1XX07Pbla90u4ffvBRevb07LS0e7q/V5o/mz9Nzy662vUcTC7v99rRjfzfeeP1V0q7zx6+yM8ua709513tO+jpx/fTs/tPHpR2v/q5t9Kzh9drnU2Vu3AwzX+OszwpABCEAgBBKAAQhAIAQSgAEIQCAEEoABCEAgBBKAAQhAIAodowcSnWxfleG6Rnt13tVfpe4ZSMR6XVbTzKV2i0de2V/nHxbfcnDz9Oz57PFqXd97/+7fTsd773J6Xd167dTM++/4Pa7nfu3SjNr+YX6dmDYjPLsvCpmHXb0u7VJv9b8OD6ldLuitsvf740f7S3f0lH0tri+fPS/OmjQi1Gvg2ltdba1cOr+eHVp18/5EkBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGAkC762bZa/01Fr9WKYSp9Rr1+viepunvV9Uq7R6N8WdK6V6ulGo1r5/DKnZ307PNnz0q7zx6epGcfPKrt/sb/+WZ69o3XvlDaffuVe6X5g538b6rf+0+/W9p949ZhevZ8p3aPHx5dT88+PK1dn1Whl+xg56C0+7QrjbeDfr4T6trhUWn3zjb/2Z8v5qXds2Wty+rT5kkBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAI6S6F3qb2jvl2kM+baoVGtRajtLtYi3FZesNpaX61Xtf+g36+RmN6mK9FaK21N+7mr8/5+aq0+xvf/l569qI9Ke3e/a//ozT/xusvp2fv/vSXSruP9vPXf1OoXGittVU/X7dyvVifMh7l50e9/HG01tpqU7tXZrNlenaxOC/tnvTy12c8vbzvq8vgSQGAIBQACEIBgCAUAAhCAYAgFAAIQgGAIBQACEIBgCAUAAhCAYDQ226328/6IAD48eBJAYAgFAAIQgGAIBQACEIBgCAUAAhCAYAgFAAIQgGA8H8BWZgO1rlvzCQAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}